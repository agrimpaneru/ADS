{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7215141c-489d-4c47-9355-6331633f51d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "\n",
    "#for reading and data-manipulation\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461e7e6b-75ac-4a9d-93e4-736efc0d9ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from wordcloud import WordCloud\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf20e5e2-9922-43f5-a699-c08f7b8c83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for data preprocessing\n",
    "import time\n",
    "from contractions import contractions_dict\n",
    "import re\n",
    "from collections import Counter\n",
    "from wordcloud import STOPWORDS\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b116e67c-3f1e-4d44-b523-f65c1cf820a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/binitkc/ADS_FiNAL/ADS/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# for ML model Implementation\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from bertopic import BERTopic\n",
    "from sklearn.metrics import silhouette_score\n",
    "import hdbscan\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e35211bc-a6ee-4cff-b1c2-82cdbc60b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "# warnings.simplefilter(\"ignore\", category=SettingWithCopyWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2702229f-ea77-4c35-b760-78439252d413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phone_url</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>domain</th>\n",
       "      <th>score</th>\n",
       "      <th>score_max</th>\n",
       "      <th>extract</th>\n",
       "      <th>author</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/cellphones/samsung-galaxy-s8/</td>\n",
       "      <td>5/2/2017</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "      <td>Verizon Wireless</td>\n",
       "      <td>verizonwireless.com</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>As a diehard Samsung fan who has had every Sam...</td>\n",
       "      <td>CarolAnn35</td>\n",
       "      <td>Samsung Galaxy S8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/cellphones/samsung-galaxy-s8/</td>\n",
       "      <td>4/28/2017</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "      <td>Phone Arena</td>\n",
       "      <td>phonearena.com</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Love the phone. the phone is sleek and smooth ...</td>\n",
       "      <td>james0923</td>\n",
       "      <td>Samsung Galaxy S8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/cellphones/samsung-galaxy-s8/</td>\n",
       "      <td>5/4/2017</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>amazon.com</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Adequate feel. Nice heft. Processor's still sl...</td>\n",
       "      <td>R. Craig</td>\n",
       "      <td>Samsung Galaxy S8 (64GB) G950U 5.8\" 4G LTE Unl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/cellphones/samsung-galaxy-s8/</td>\n",
       "      <td>5/2/2017</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>samsung.com</td>\n",
       "      <td>9.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Never disappointed. One of the reasons I've be...</td>\n",
       "      <td>Buster2020</td>\n",
       "      <td>Samsung Galaxy S8 64GB (AT&amp;T)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/cellphones/samsung-galaxy-s8/</td>\n",
       "      <td>5/11/2017</td>\n",
       "      <td>en</td>\n",
       "      <td>us</td>\n",
       "      <td>Verizon Wireless</td>\n",
       "      <td>verizonwireless.com</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>I've now found that i'm in a group of people t...</td>\n",
       "      <td>S Ate Mine</td>\n",
       "      <td>Samsung Galaxy S8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        phone_url       date lang country            source  \\\n",
       "0  /cellphones/samsung-galaxy-s8/   5/2/2017   en      us  Verizon Wireless   \n",
       "1  /cellphones/samsung-galaxy-s8/  4/28/2017   en      us       Phone Arena   \n",
       "2  /cellphones/samsung-galaxy-s8/   5/4/2017   en      us            Amazon   \n",
       "3  /cellphones/samsung-galaxy-s8/   5/2/2017   en      us           Samsung   \n",
       "4  /cellphones/samsung-galaxy-s8/  5/11/2017   en      us  Verizon Wireless   \n",
       "\n",
       "                domain  score  score_max  \\\n",
       "0  verizonwireless.com   10.0       10.0   \n",
       "1       phonearena.com   10.0       10.0   \n",
       "2           amazon.com    6.0       10.0   \n",
       "3          samsung.com    9.2       10.0   \n",
       "4  verizonwireless.com    4.0       10.0   \n",
       "\n",
       "                                             extract       author  \\\n",
       "0  As a diehard Samsung fan who has had every Sam...   CarolAnn35   \n",
       "1  Love the phone. the phone is sleek and smooth ...    james0923   \n",
       "2  Adequate feel. Nice heft. Processor's still sl...     R. Craig   \n",
       "3  Never disappointed. One of the reasons I've be...  Buster2020    \n",
       "4  I've now found that i'm in a group of people t...   S Ate Mine   \n",
       "\n",
       "                                             product  \n",
       "0                                  Samsung Galaxy S8  \n",
       "1                                  Samsung Galaxy S8  \n",
       "2  Samsung Galaxy S8 (64GB) G950U 5.8\" 4G LTE Unl...  \n",
       "3                      Samsung Galaxy S8 64GB (AT&T)  \n",
       "4                                  Samsung Galaxy S8  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Cell_Phone_Reviews/phone_user_review_file_1.csv', encoding='ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "887b9954-b4ae-4097-8e77-2dd8db89f64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 374910 entries, 0 to 374909\n",
      "Data columns (total 11 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   phone_url  374910 non-null  object \n",
      " 1   date       374910 non-null  object \n",
      " 2   lang       374910 non-null  object \n",
      " 3   country    374910 non-null  object \n",
      " 4   source     374910 non-null  object \n",
      " 5   domain     374910 non-null  object \n",
      " 6   score      366691 non-null  float64\n",
      " 7   score_max  366691 non-null  float64\n",
      " 8   extract    371934 non-null  object \n",
      " 9   author     371630 non-null  object \n",
      " 10  product    374910 non-null  object \n",
      "dtypes: float64(2), object(9)\n",
      "memory usage: 31.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01ee2e6a-abc7-4b2c-b52c-bcc8ad6e1544",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['product'].str.contains('Samsung', na=False)]\n",
    "df = df[df['lang']=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2d95d7b-b89c-43d3-a018-7fd015aa293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 64368 entries, 0 to 369253\n",
      "Data columns (total 11 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   phone_url  64368 non-null  object \n",
      " 1   date       64368 non-null  object \n",
      " 2   lang       64368 non-null  object \n",
      " 3   country    64368 non-null  object \n",
      " 4   source     64368 non-null  object \n",
      " 5   domain     64368 non-null  object \n",
      " 6   score      64359 non-null  float64\n",
      " 7   score_max  64359 non-null  float64\n",
      " 8   extract    64162 non-null  object \n",
      " 9   author     64331 non-null  object \n",
      " 10  product    64368 non-null  object \n",
      "dtypes: float64(2), object(9)\n",
      "memory usage: 5.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13bc9c16-d7ea-48e1-a37e-30ed366a9117",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['phone_url', 'score', 'extract']].dropna()\n",
    "df.rename(columns={'phone_url': 'unique_identifier', 'score': 'score', 'extract': 'brief_review'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf13c39a-b04c-47dd-b6f8-8bbfc4b1d416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_identifier</th>\n",
       "      <th>score</th>\n",
       "      <th>brief_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/cellphones/samsung-galaxy-s8/</td>\n",
       "      <td>10.0</td>\n",
       "      <td>As a diehard Samsung fan who has had every Sam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/cellphones/samsung-galaxy-s8/</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Love the phone. the phone is sleek and smooth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/cellphones/samsung-galaxy-s8/</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Adequate feel. Nice heft. Processor's still sl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/cellphones/samsung-galaxy-s8/</td>\n",
       "      <td>9.2</td>\n",
       "      <td>Never disappointed. One of the reasons I've be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/cellphones/samsung-galaxy-s8/</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I've now found that i'm in a group of people t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                unique_identifier  score  \\\n",
       "0  /cellphones/samsung-galaxy-s8/   10.0   \n",
       "1  /cellphones/samsung-galaxy-s8/   10.0   \n",
       "2  /cellphones/samsung-galaxy-s8/    6.0   \n",
       "3  /cellphones/samsung-galaxy-s8/    9.2   \n",
       "4  /cellphones/samsung-galaxy-s8/    4.0   \n",
       "\n",
       "                                        brief_review  \n",
       "0  As a diehard Samsung fan who has had every Sam...  \n",
       "1  Love the phone. the phone is sleek and smooth ...  \n",
       "2  Adequate feel. Nice heft. Processor's still sl...  \n",
       "3  Never disappointed. One of the reasons I've be...  \n",
       "4  I've now found that i'm in a group of people t...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7430aa60-e344-41f2-941f-ebc8e99f005a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unique_identifier    0\n",
       "score                0\n",
       "brief_review         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handling missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d26ae0e9-c01e-4a59-967c-d57df1ad7136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "861"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#handling duplicate values \n",
    "len(df[df.duplicated(subset=['brief_review'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c8c53c2-0560-4486-8e92-df3f000029ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64108"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.duplicated(subset=['unique_identifier'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbafc310-1916-49c9-b27f-117079667925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                unique_identifier  score  \\\n",
      "0  /cellphones/samsung-galaxy-s8/   10.0   \n",
      "1  /cellphones/samsung-galaxy-s8/   10.0   \n",
      "2  /cellphones/samsung-galaxy-s8/    6.0   \n",
      "3  /cellphones/samsung-galaxy-s8/    9.2   \n",
      "4  /cellphones/samsung-galaxy-s8/    4.0   \n",
      "\n",
      "                                        brief_review  \n",
      "0  As a diehard Samsung fan who has had every Sam...  \n",
      "1  Love the phone. the phone is sleek and smooth ...  \n",
      "2  Adequate feel. Nice heft. Processor's still sl...  \n",
      "3  Never disappointed. One of the reasons I've be...  \n",
      "4  I've now found that i'm in a group of people t...  \n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate \n",
    "df_cleaned = df.drop_duplicates(subset = [\"brief_review\", \"unique_identifier\"])\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d4d906f-f710-4fd0-9f87-7eb58b677f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 63611 entries, 0 to 369253\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   unique_identifier  63611 non-null  object \n",
      " 1   score              63611 non-null  float64\n",
      " 2   brief_review       63611 non-null  object \n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3465f55d-c1d7-40d5-8330-fc075f6accb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f27da-5e5c-4b94-9102-74aec64db235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f6f4ec-2384-4132-a9d9-460ba4afdbad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3cab200-6020-4167-aad0-98d9647e3362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand Contraction\n",
    "\n",
    "# Function to expand contractions using the contractions_dict\n",
    "def expand_contractions(text):\n",
    "    # Regular expression pattern to match contractions\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
    "                                      flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        expanded = contractions_dict.get(match.lower())\n",
    "        return expanded\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    return expanded_text\n",
    "\n",
    "# Apply the expand_contractions function to the \"brief_review\" column\n",
    "df_cleaned['brief_review'] = df_cleaned['brief_review'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e590107-2423-424a-8454-9dda6ba1e83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1f0c4e1-726c-4a77-af9d-13e82d307e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text in the \"brief_review\" column to lowercase\n",
    "df_cleaned['brief_review'] = df_cleaned['brief_review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2961201a-3dbb-44e4-a916-252427ff3b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa6fe313-dc48-4a2c-a95d-b7d8dd563513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuations\n",
    "\n",
    "# Function to remove punctuations from text\n",
    "def remove_punctuations(text):\n",
    "    # Create a translation table to remove punctuations\n",
    "    translator = str.maketrans('', '', string.punctuation +'\\n')\n",
    "\n",
    "    # Apply the translation table to remove punctuations\n",
    "    text_without_punctuations = text.translate(translator)\n",
    "    return text_without_punctuations\n",
    "\n",
    "# Apply the remove_punctuations function to the \"brief_review\" column\n",
    "df_cleaned['brief_review'] = df_cleaned['brief_review'].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f30664-0917-422d-a86f-097b65305fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79f4e010-7539-43a5-8f36-6d33ab2239f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    as a diehard samsung fan who has had every sam...\n",
       "1    love the phone the phone is sleek and smooth a...\n",
       "2    adequate feel nice heft processors still slugg...\n",
       "3    never disappointed one of the reasons been a l...\n",
       "4    now found that in a group of people that have ...\n",
       "Name: brief_review, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to remove URLs from text\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return re.sub(url_pattern, '', text)\n",
    "\n",
    "# Function to remove words containing digits from text\n",
    "def remove_words_with_digits(text):\n",
    "    return ' '.join(word for word in text.split() if not any(char.isdigit() for char in word))\n",
    "\n",
    "# Function to remove non-ASCII characters (special characters)\n",
    "def remove_special_characters(text):\n",
    "    # Replace non-ASCII characters with a space\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "# Apply the remove_urls function to the \"brief_review\" column\n",
    "df_cleaned['brief_review'] = df_cleaned['brief_review'].apply(remove_urls)\n",
    "\n",
    "# Apply the remove_words_with_digits function to the \"brief_review\" column\n",
    "df_cleaned['brief_review'] = df_cleaned['brief_review'].apply(remove_words_with_digits)\n",
    "\n",
    "# Apply the remove_special_characters function to the \"brief_review\" column\n",
    "df_cleaned['brief_review'] = df_cleaned['brief_review'].apply(remove_special_characters)\n",
    "\n",
    "# Verify the cleaned data\n",
    "df_cleaned['brief_review'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed0fa25-5224-4c5d-896c-83b90b7f2e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11f2a950-95dd-480b-ae78-9f18664dec85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    as a diehard samsung fan who has had every sam...\n",
       "1    love the phone the phone is sleek and smooth a...\n",
       "2    adequate feel nice heft processors still slugg...\n",
       "3    never disappointed one of the reasons been a l...\n",
       "4    now found that in a group of people that have ...\n",
       "Name: brief_review, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS IS THE CODE TO REMOVE THE REVIEWS WITH WORD COUNT<=5\n",
    "# Function to filter reviews based on word count\n",
    "def filter_short_reviews(text, min_word_count=5):\n",
    "    return len(text.split()) >= min_word_count\n",
    "\n",
    "# Apply the filter function to the \"brief_review\" column and keep only reviews with word count >= 5\n",
    "df_cleaned = df_cleaned[df_cleaned['brief_review'].apply(filter_short_reviews)]\n",
    "\n",
    "# Verify the filtered data\n",
    "df_cleaned['brief_review'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38dc498-b417-496b-9662-70b35d8498e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78a26a5c-ef23-458f-8c0c-78339d711db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/binitkc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    " # Remove Stopwords\n",
    "# Download the list of stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add unnecessary words to the list of English stopwords\n",
    "stop_unnecessary_words = stop_words.union(set(['mr', 'people', 'would', 'year', 'said', 'say', 'also', 'wale', 'could', 'chars']))\n",
    "\n",
    "\n",
    "# Function to remove stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    words = [word for word in words if len(word)>2]\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_unnecessary_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply the remove_stopwords function to the \"brief_review\" column\n",
    "df_cleaned['brief_review'] = df_cleaned['brief_review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4e96b5-bfcd-41f7-80a7-0491b7dacc18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c9050f0-fbb4-4455-a732-63dc4ed25f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove white spaces from the \"brief_review\" column\n",
    "df_cleaned['brief_review'] = df_cleaned['brief_review'].str.replace('\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc26a51-a5a1-4e16-ae76-5fc813f3e450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2a42a3a-d767-43d5-975a-126dacb50ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_identifier</th>\n",
       "      <th>score</th>\n",
       "      <th>brief_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/cellphones/samsung-galaxy-grand-prime/</td>\n",
       "      <td>6.0</td>\n",
       "      <td>volume bit low overall good phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/cellphones/samsung-galaxy-s6-edge-sm-g925f/</td>\n",
       "      <td>10.0</td>\n",
       "      <td>good design delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/cellphones/samsung-galaxy-s6-edge-sm-g925f/</td>\n",
       "      <td>9.6</td>\n",
       "      <td>samsung galaxy edge phone sleek smooth quality...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/cellphones/samsung-galaxy-note-4/</td>\n",
       "      <td>9.0</td>\n",
       "      <td>midst researching next sphone found upgrade ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/cellphones/samsung-galaxy-s6/</td>\n",
       "      <td>10.0</td>\n",
       "      <td>really happy product thank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              unique_identifier  score  \\\n",
       "0       /cellphones/samsung-galaxy-grand-prime/    6.0   \n",
       "1  /cellphones/samsung-galaxy-s6-edge-sm-g925f/   10.0   \n",
       "2  /cellphones/samsung-galaxy-s6-edge-sm-g925f/    9.6   \n",
       "3            /cellphones/samsung-galaxy-note-4/    9.0   \n",
       "4                /cellphones/samsung-galaxy-s6/   10.0   \n",
       "\n",
       "                                        brief_review  \n",
       "0                  volume bit low overall good phone  \n",
       "1                               good design delivery  \n",
       "2  samsung galaxy edge phone sleek smooth quality...  \n",
       "3  midst researching next sphone found upgrade ve...  \n",
       "4                         really happy product thank  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the data\n",
    "df_shuffled = df_cleaned.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d95589-0637-466b-b763-fb602d7f6787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7fafc5a-3550-4326-91a8-92b795539258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 60712 entries, 0 to 369253\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   unique_identifier  60712 non-null  object \n",
      " 1   score              60712 non-null  float64\n",
      " 2   brief_review       60712 non-null  object \n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fcf3dd-d9dc-4278-b589-faa7ead9bc35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e1c728-99f7-4a04-9eed-4a5cb611539c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d01ee15-2a3b-47de-8de1-76960afef233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to convert the df (series) to list for passing to BERTopic.\n",
    "sentenceList = df_cleaned[\"brief_review\"].tolist()\n",
    "samplesentenceList = sentenceList[1:10001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "baa178d9-b89e-443f-88b4-a63536dd2c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Embed the Texts\n",
    "embedding_model = SentenceTransformer('paraphrase-MiniLM-L3-v2')\n",
    "embeddings = embedding_model.encode(samplesentenceList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9087a995-f48c-41a0-8e15-8828520b48a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Dimensionality Reduction using UMAP\n",
    "# Option 1: UMAP\n",
    "umap_model = UMAP(n_components=5, random_state=42)\n",
    "reduced_embeddings = umap_model.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d780360d-1453-407b-a95a-b32f4c0c857e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans Silhouette Score: 0.35315001010894775\n"
     ]
    }
   ],
   "source": [
    "# KMeans Clustering\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(reduced_embeddings)\n",
    "kmeans_silhouette = silhouette_score(reduced_embeddings, kmeans_labels)\n",
    "print(f\"KMeans Silhouette Score: {kmeans_silhouette}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1950ae6-1e09-43ca-bc06-a3b85714c9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDBSCAN Silhouette Score: 0.3159528076648712\n"
     ]
    }
   ],
   "source": [
    "# HDBSCAN Clustering\n",
    "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=1, cluster_selection_method='eom')\n",
    "hdbscan_labels = hdbscan_model.fit_predict(reduced_embeddings)\n",
    "\n",
    "# Note: Silhouette score works best when there are multiple clusters, and HDBSCAN may assign some points as noise (-1 label).\n",
    "# Filter out noise points for Silhouette calculation\n",
    "filtered_embeddings = reduced_embeddings[hdbscan_labels != -1]\n",
    "filtered_labels = hdbscan_labels[hdbscan_labels != -1]\n",
    "\n",
    "# Only calculate silhouette score if there are clusters\n",
    "if len(set(filtered_labels)) > 1:\n",
    "    hdbscan_silhouette = silhouette_score(filtered_embeddings, filtered_labels)\n",
    "    print(f\"HDBSCAN Silhouette Score: {hdbscan_silhouette}\")\n",
    "else:\n",
    "    print(\"HDBSCAN resulted in only one cluster or noise.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce3511be-9957-4d18-9a25-9966677c791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter Tuning.\n",
    "\n",
    "\n",
    "# Step 2: Define hyperparameter grids\n",
    "kmeans_params = {'n_clusters': [2, 3, 5, 6,  7, 10, 15, 20, 25, 30, 35, 40, 45, 50]}\n",
    "hdbscan_params = {\n",
    "    'min_cluster_size': [5, 6, 8, 10, 15, 20, 25, 30],\n",
    "    'min_samples': [1, 5, 8, 10, 12, 15],\n",
    "    'cluster_selection_method': ['eom', 'leaf']\n",
    "}\n",
    "\n",
    "# Step 3: Evaluate KMeans\n",
    "kmeans_results = []\n",
    "for n_clusters in kmeans_params['n_clusters']:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(reduced_embeddings)\n",
    "    score = silhouette_score(reduced_embeddings, labels)\n",
    "    kmeans_results.append({'Algorithm': 'KMeans', 'n_clusters': n_clusters, 'Silhouette Score': score})\n",
    "\n",
    "# Step 4: Evaluate HDBSCAN\n",
    "hdbscan_results = []\n",
    "for min_cluster_size in hdbscan_params['min_cluster_size']:\n",
    "    for min_samples in hdbscan_params['min_samples']:\n",
    "        for method in hdbscan_params['cluster_selection_method']:\n",
    "            hdbscan_model = hdbscan.HDBSCAN(\n",
    "                min_cluster_size=min_cluster_size,\n",
    "                min_samples=min_samples,\n",
    "                cluster_selection_method=method\n",
    "            )\n",
    "            labels = hdbscan_model.fit_predict(reduced_embeddings)\n",
    "            # Skip noise points for Silhouette Score calculation\n",
    "            if len(set(labels)) > 1:  # Ensure there are clusters\n",
    "                score = silhouette_score(reduced_embeddings, labels)\n",
    "                hdbscan_results.append({\n",
    "                    'Algorithm': 'HDBSCAN',\n",
    "                    'min_cluster_size': min_cluster_size,\n",
    "                    'min_samples': min_samples,\n",
    "                    'cluster_selection_method': method,\n",
    "                    'Silhouette Score': score\n",
    "                })\n",
    "\n",
    "# Step 5: Combine results and display in a table\n",
    "all_results = pd.DataFrame(kmeans_results + hdbscan_results)\n",
    "sorted_results = all_results.sort_values(by='Silhouette Score', ascending=False)\n",
    "\n",
    "# print(sorted_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25e390a8-a001-4bd6-b8fc-0a7757f2cf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Result:\n",
      "Algorithm                     KMeans\n",
      "n_clusters                       6.0\n",
      "Silhouette Score            0.376034\n",
      "min_cluster_size                 NaN\n",
      "min_samples                      NaN\n",
      "cluster_selection_method         NaN\n",
      "Name: 3, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print the best result\n",
    "best_result = sorted_results.iloc[0]\n",
    "print(\"\\nBest Result:\")\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326a2d87-8fc0-42c9-b46a-1463f38645d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "231d3d52-73a4-4d3a-9a29-c0a96911f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define KMeans model with the best parameters (e.g., n_clusters=5 from your tuning results)\n",
    "kmeans_model = KMeans(\n",
    "    n_clusters=6,  # Best parameter based on your tuning\n",
    "    random_state=42,\n",
    "    prediction_data=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2016930-5ae8-41eb-ad49-48b3d9ec4fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(\n",
    "    embedding_model=\"paraphrase-MiniLM-L3-v2\",  # or another pre-trained model\n",
    "    hdbscan_model=kmeans_model,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "92ffcaf0-9a97-47d1-a0ca-fdfb5510b391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 10:24:41,009 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0bf75f29b8480bac8f8393142dcfd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 10:24:49,042 - BERTopic - Embedding - Completed ✓\n",
      "2024-11-17 10:24:49,042 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-11-17 10:24:56,857 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-11-17 10:24:56,858 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-11-17 10:24:56,879 - BERTopic - Cluster - Completed ✓\n",
      "2024-11-17 10:24:56,883 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-11-17 10:24:56,960 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "topics, probs = topic_model.fit_transform(samplesentenceList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b33971-6be4-40c3-aa22-85314a86384e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1a5d62c-23e5-49d3-bfd4-a6f67b57fec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/binitkc/ADS_FiNAL/ADS/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/binitkc/ADS_FiNAL/ADS/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/binitkc/ADS_FiNAL/ADS/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/binitkc/ADS_FiNAL/ADS/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/binitkc/ADS_FiNAL/ADS/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/binitkc/ADS_FiNAL/ADS/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/binitkc/ADS_FiNAL/ADS/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.46803253675543327\n"
     ]
    }
   ],
   "source": [
    "# Now retrieve topics and their words\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "topic_words = []\n",
    "for topic_id in range(len(topic_model.get_topics())):\n",
    "    # `get_topic` should return a list of word-score pairs for each topic\n",
    "    topic = topic_model.get_topic(topic_id)\n",
    "    if topic:  # Make sure the topic is not empty\n",
    "        words = [word for word, _ in topic]\n",
    "        topic_words.append(words)\n",
    "\n",
    "# Tokenize the original documents for coherence calculation\n",
    "tokenized_docs = [doc.split() for doc in samplesentenceList]\n",
    "\n",
    "# Create a dictionary and calculate coherence\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "coherence_model = CoherenceModel(\n",
    "    topics=topic_words,\n",
    "    texts=tokenized_docs,\n",
    "    dictionary=dictionary,\n",
    "    coherence=\"c_v\"\n",
    ")\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d620b-614d-4f5b-9785-5460211baeba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22d389c7-4891-4cde-9b5c-1bd9fb95358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Diversity Score: 0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "def topic_diversity_score(topic_model, top_n_words=10):\n",
    "    all_words = []\n",
    "    for topic in topic_model.get_topics().values():\n",
    "        words = [word for word, _ in topic[:top_n_words]]\n",
    "        all_words.extend(words)\n",
    "    unique_words = set(all_words)\n",
    "    return len(unique_words) / len(all_words)\n",
    "\n",
    "diversity_score = topic_diversity_score(topic_model)\n",
    "print(f\"Topic Diversity Score: {diversity_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd339a85-6f56-4a6b-a01e-1ffa28cc4dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ece01c6e-95ec-4e25-97ff-ad0ea6cf1b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 10:25:13,965 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    }
   ],
   "source": [
    "# Specify the path where you want to save the model\n",
    "model_path = \"Trained_models/bertopic_samsung_model\"\n",
    "\n",
    "# Save the trained BERTopic model\n",
    "topic_model.save(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302f645-3aa8-4a20-a065-c8c0a60b2946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1cdd2038-40fa-4b63-8077-723708297c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"Trained_models/bertopic_samsung_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0acf1453-adda-4d5b-9918-9fb91726aa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved BERTopic model\n",
    "topic_model = BERTopic.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23a3dad1-13d8-431a-ad9b-977668ce3ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model calculate_probabilities: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model calculate_probabilities: {topic_model.calculate_probabilities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1620970c-502a-4590-9470-31d723c2da75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550d7653115049b898d4e1085bee4ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 10:25:39,092 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2024-11-17 10:25:39,555 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-11-17 10:25:39,556 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2024-11-17 10:25:39,561 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m topics \u001b[38;5;241m=\u001b[39m topic_model\u001b[38;5;241m.\u001b[39mget_topic_info()  \u001b[38;5;66;03m# Get topic names\u001b[39;00m\n\u001b[1;32m      9\u001b[0m topic_labels \u001b[38;5;241m=\u001b[39m topics\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m---> 10\u001b[0m topic_labels \u001b[38;5;241m=\u001b[39m {k: topic_labels[k] \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m topic_labels \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mprobabilities\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m))}\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Prepare data for visualization\u001b[39;00m\n\u001b[1;32m     13\u001b[0m top_topics \u001b[38;5;241m=\u001b[39m {topic_labels[i]: prob \u001b[38;5;28;01mfor\u001b[39;00m i, prob \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(probabilities[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m prob \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.01\u001b[39m}  \u001b[38;5;66;03m# Filter significant probabilities\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example new document\n",
    "new_document = \"The camera is good \"\n",
    "\n",
    "# Get the topic and probability distribution\n",
    "topic, probabilities = topic_model.transform([new_document])\n",
    "\n",
    "# Extract topic names and probabilities\n",
    "topics = topic_model.get_topic_info()  # Get topic names\n",
    "topic_labels = topics.set_index(\"Topic\")[\"Name\"].to_dict()\n",
    "topic_labels = {k: topic_labels[k] if k in topic_labels else f\"Topic {k}\" for k in range(len(probabilities[0]))}\n",
    "\n",
    "# Prepare data for visualization\n",
    "top_topics = {topic_labels[i]: prob for i, prob in enumerate(probabilities[0]) if prob > 0.01}  # Filter significant probabilities\n",
    "labels = list(top_topics.keys())\n",
    "values = list(top_topics.values())\n",
    "\n",
    "# Plot pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"Topic Distribution for New Document\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d61976-8017-49a8-89da-bbd066390517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d811474c-8ded-47d3-8b85-cb9cc1637cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d50934-0457-45da-827b-e9186e4fd802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7ef421-63bf-4b10-aeb4-4b476a09be26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADS",
   "language": "python",
   "name": "ads"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
